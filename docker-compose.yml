services:
  matching-service:
    build:
      context: .
      dockerfile: Dockerfile
    image: matching-service:latest
    container_name: matching-service
    ports:
      - "${API_PORT:-8000}:8000"
    volumes:
      # Persistent data storage
      - ./data:/app/data:rw
      # Cache for ML models (speeds up container startup)
      - ml-models-cache:/app/.cache:rw
    environment:
      # Python settings
      - PYTHONUNBUFFERED=1
      - PYTHONDONTWRITEBYTECODE=1
      
      # API Configuration (API_*)
      - API_HOST=${API_HOST:-0.0.0.0}
      - API_PORT=${API_PORT:-8000}
      - API_RELOAD=${API_RELOAD:-false}
      - API_DEFAULT_TOP_K=${API_DEFAULT_TOP_K:-5}
      - API_MAX_TOP_K=${API_MAX_TOP_K:-50}
      
      # Database Configuration (DB_*)
      - DB_VECTOR_DB_PATH=${DB_VECTOR_DB_PATH:-data/vectors.db}
      
      # ML Model Configuration (ML_*)
      - ML_MODEL_NAME=${ML_MODEL_NAME:-sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2}
      - ML_DEVICE=${ML_DEVICE:-cpu}
      - ML_VECTOR_DIM=${ML_VECTOR_DIM:-384}
      - ML_EMBEDDING_BATCH_SIZE=${ML_EMBEDDING_BATCH_SIZE:-32}
      - ML_MAX_TEXT_LENGTH=${ML_MAX_TEXT_LENGTH:-512}
      
      # Logging Configuration (LOG_*)
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      
      # Performance tuning for NumPy/PyTorch on CPU
      - OMP_NUM_THREADS=${OMP_NUM_THREADS:-4}
      - MKL_NUM_THREADS=${MKL_NUM_THREADS:-4}
      - OPENBLAS_NUM_THREADS=${OPENBLAS_NUM_THREADS:-4}
      - NUMEXPR_NUM_THREADS=${NUMEXPR_NUM_THREADS:-4}
      - TORCH_NUM_THREADS=${TORCH_NUM_THREADS:-4}
    restart: on-failure:3
    # Healthcheck is defined in Dockerfile
    # Resource limits (adjust based on your hardware)
    deploy:
      resources:
        limits:
          cpus: '${CPU_LIMIT:-4}'
          memory: ${MEMORY_LIMIT:-4G}
        reservations:
          cpus: '${CPU_RESERVE:-2}'
          memory: ${MEMORY_RESERVE:-2G}
    # Security options
    security_opt:
      - no-new-privileges:true
    # Temporary filesystems
    tmpfs:
      - /tmp:noexec,nosuid,size=100m
    # Logging
    logging:
      driver: "json-file"
      options:
        max-size: "100m"
        max-file: "3"

volumes:
  ml-models-cache:
    driver: local

