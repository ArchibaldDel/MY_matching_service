services:
  matching-service:
    build:
      context: .
      dockerfile: Dockerfile
      # Enable BuildKit caching
      cache_from:
        - type=registry,ref=matching-service:buildcache
    image: matching-service:latest
    container_name: matching-service
    ports:
      - "${API_PORT:-8000}:8000"
    volumes:
      # Persistent data storage
      - ./data:/app/data:rw
      # Cache for ML models (speeds up container startup)
      - ml-models-cache:/app/.cache:rw
    environment:
      # Python settings
      - PYTHONUNBUFFERED=1
      - PYTHONDONTWRITEBYTECODE=1
      # API configuration
      - API_HOST=${API_HOST:-0.0.0.0}
      - API_PORT=${API_PORT:-8000}
      # PyTorch device: "cpu", "cuda", or unset (auto-detect)
      - TORCH_DEVICE=cpu
      # Performance tuning for NumPy/PyTorch on CPU
      - OMP_NUM_THREADS=${OMP_NUM_THREADS:-4}
      - MKL_NUM_THREADS=${MKL_NUM_THREADS:-4}
      - OPENBLAS_NUM_THREADS=${OPENBLAS_NUM_THREADS:-4}
      - NUMEXPR_NUM_THREADS=${NUMEXPR_NUM_THREADS:-4}
      - TORCH_NUM_THREADS=${TORCH_NUM_THREADS:-4}
    restart: unless-stopped
    # Healthcheck is defined in Dockerfile
    # Resource limits (adjust based on your hardware)
    deploy:
      resources:
        limits:
          cpus: '${CPU_LIMIT:-4}'
          memory: ${MEMORY_LIMIT:-4G}
        reservations:
          cpus: '${CPU_RESERVE:-2}'
          memory: ${MEMORY_RESERVE:-2G}
    # Security options
    security_opt:
      - no-new-privileges:true
    # Read-only root filesystem (except mounted volumes)
    read_only: true
    tmpfs:
      - /tmp:noexec,nosuid,size=100m

volumes:
  ml-models-cache:
    driver: local

